from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import os
import math
import h5py



class ITrainable:
    def __init__(self):
        self.is_train = False

    def set_train(self, is_train):
        self.is_train = is_train

    def regularization_cost(self,m):
        return 0

    def forward_propagation(self, X):
        raise NotImplementedError("forward_propagation not implemented: ITrainable is an interface")

    def backward_propagation(self, dY_hat):
        raise NotImplementedError("backward_propagation not implemented: ITrainable is an interface")

    def update_parameters(self, t = 1):
        raise NotImplementedError("update_parameters not implemented: ITrainable is an interface")
    
    def save_parameters(self, file_path):
        pass

    def restore_parameters(self, file_path):
        pass

    def parms_to_vec(self):
        pass
    def vec_to_parms(self, vec):
        pass
    def gradients_to_vec(self):
        pass
    def to_layers(self):
        return np.array([self])






class DLNetwork(ITrainable):
    def __init__(self, name):
        ITrainable.__init__(self)
        self.name = name
        self.layers = []

    def __str__(self): 
        s= self.name+"\n"
        for l in self.layers:
            s+=str(l)
        return s


    def add(self,iTrainable): 
        for l in self.layers:
            if l.name== iTrainable.name:
                raise ValueError("The name allredey exists")
        self.layers.append(iTrainable)

    def forward_propagation(self, X):
        a=X
        for l in self.layers:
           a=l.forward_propagation(a)
        return a
    def backward_propagation(self, dY_hat): 
        da=dY_hat
        for l in reversed(self.layers):
           da=l.backward_propagation(da)
        return da
    def update_parameters(self, t = 1): 
        for l in self.layers:
           l.update_parameters()

    def save_parameters(self, directory_path):
        path=os.path.join(directory_path,self.name)
        os.makedirs(path, exist_ok=True)
        for layer in self.layers:
            layer.save_parameters(path)

    def restore_parameters(self, directory_path):
        path=os.path.join(directory_path,self.name)
        os.makedirs(path, exist_ok=True)
        for layer in self.layers:
            layer.restore_parameters(path)   

    def to_layers(self):
        layers = np.array([])
        for l in self.layers:
                layers = np.concatenate((layers,l.to_layers()))
        return layers

    def set_train(self, is_train):
        for l in self.layers:
            l.set_train(is_train)

    def regularization_cost(self, m):
        sum = 0
        for l in self.layers:
            sum += l.regularization_cost(m)
        return sum


class DLActivation(ITrainable):
    def __init__(self, activation): 
        ITrainable.__init__(self)
        self.name = activation
        if (activation == "sigmoid"):
            self.forward_propagation = self.sigmoid
            self.backward_propagation = self.sigmoid_dZ
        elif (activation == "relu"):
            self.forward_propagation = self.relu
            self.backward_propagation = self.relu_dZ
        elif (activation == "tanh"):
            self.forward_propagation = self.tanh
            self.backward_propagation = self.tanh_dZ
        elif (activation == "leaky_relu"):
            self.forward_propagation = self.leaky_relu
            self.backward_propagation = self.leaky_relu_dZ
            self.leaky_relu_d = 0.01
        elif (activation == "softmax"):
            self.forward_propagation = self.softmax
            self.backward_propagation = self.softmax_dZ
        else:
            raise NotImplementedError("(unImplementedError activation:, error)")


    def __str__(self):
        s = "Activation function: "+self.name
        if self.name =="leaky_relu":
            s += f"\td = {self.leaky_relu_d}"
        return s
    def sigmoid(self, Z): 
        self.s= 1/(1+np.e**(-Z))
        return self.s
    def sigmoid_dZ(self, dS):
        return dS*(self.s*(1-self.s))
    def update_parameters(self,t = 1): 
        pass

    def tanh(self, Z):
        self.Z = Z
        self.A = np.tanh(Z)
        return self.A
    def tanh_dZ(self, dA):
        dAdZ = (1-self.A**2)
        return dAdZ*dA 
    def relu(self, Z):
        self.Z = Z
        self.A = np.maximum(0,Z)
        return self.A
    def relu_dZ(self, dA):
        dAdZ = np.where(self.Z <= 0, 0, 1)
        return dAdZ*dA 
    def leaky_relu(self, Z):
        self.Z = Z
        self.A = np.where(self.Z <= 0, self.leaky_relu_d*Z, Z)
        return self.A
    def leaky_relu_dZ(self, dA): 
        dAdZ = np.where(self.Z <= 0, self.leaky_relu_d, 1)
        return dAdZ*dA 

    def softmax(self, z):
        a = np.e**z
        a = np.sum(a,keepdims=True,axis=0)
        a = np.e**z/a
        return a

    def softmax_dZ(self, dZ):
        return dZ





class DLLinearLayer(ITrainable):
    def __init__(self,name,num_units,input_size, alpha,optimization=None,regularization=None):
        ITrainable.__init__(self)
        self.regularization=regularization
        self.num_units=num_units
        self.input_size=input_size
        self.name = name
        self.alpha = alpha
        self.W = np.empty((self.num_units,self.input_size))
        self.optimization = optimization
        self.W_He_initialization()  
        self.b = np.zeros((num_units,1))
        # optimization parameters
        if regularization=="L2":
            self.L2_lambda =0.6
        if regularization=="dropout":
            self.dropout_keep_prob=0.7
        if self.optimization == 'adaptive':
            self.adaptive_cont = 1.1
            self.adaptive_switch = 0.5
            self.adaptive_W = np.full(self.W.shape,alpha,dtype=float)
            self.adaptive_b = np.full(self.b.shape,alpha,dtype=float)
        if optimization == 'adam':
            self.adam_v_db = np.zeros((num_units,1))
            self.adam_v_dW = np.zeros((num_units,1)) 
            self.adam_s_dW = np.zeros((num_units,1))  
            self.adam_s_db = np.zeros((num_units,1)) 
            self.adam_beta1 = 0.9
            self.adam_beta2 = 0.999
            self.adam_epsilon = 1.0e-8
            

    def W_He_initialization(self):
        self.W = DLLinearLayer.normal_initialization(self.W.shape,np.sqrt(2/self.input_size))

    def W_Xaviar_initialization(self):
        self.W = DLLinearLayer.normal_initialization(self.W.shape,np.sqrt(1/self.input_size))

    def __str__(self):
        s = f"{self.name} Function:\n"
        s += f"\tlearning_rate (alpha): {self.alpha}\n"
        s += f"\t\tinput_size: {self.input_size}\n"
        s += f"\t\tnum_units: {self.num_units}\n"
        if self.optimization != None:
            s += f"\tOptimization: {self.optimization}\n"
            if self.optimization == "adaptive":
                s += f"\t\tadaptive parameters:\n"
                s += f"\t\t\tcont: {self.adaptive_cont}\n"
                s += f"\t\t\tswitch: {self.adaptive_switch}\n"
        s += "\tParameters shape:\n"
        s += f"\t\tW shape: {self.W.shape}\n"
        s += f"\t\tb shape: {self.b.shape}\n"
        s+= self.regularization_str()
        return s;


    def forward_propagation(self, prev_A):
        self.prev_A = self.forward_dropout(prev_A)
        Z = self.W @ self.prev_A+self.b
        return Z



    def backward_propagation(self, dZ):
        db_m_values = dZ * np.full((1,self.prev_A.shape[1]),1)
        self.db = np.sum(db_m_values, keepdims=True, axis=1)
        self.dW = dZ@self.prev_A.T
        m=self.db.shape[0]
        self.dA_prev =self.W.T @ dZ
        self.backward_dropout(self.dA_prev)
        m=dZ.shape[-1]
        if self.regularization=="L2":
            self.dW+=self.L2_lambda*self.W/m
        return self.dA_prev 


    def update_parameters(self, t = 1):
        if self.optimization == 'adaptive':
            self.adaptive_W *= np.where(self.adaptive_W * self.dW > 0, self.adaptive_cont, -self.adaptive_switch)
            self.W -= self.adaptive_W 
            #if self.adaptive_b * self.db > 0:
            #    self.adaptive_b *= self.adaptive_cont
            #else:
            #    self.adaptive_b *= -self.adaptive_switch
            #self.b -= self.adaptive_b 
            self.adaptive_b *= np.where(self.adaptive_b * self.db > 0, self.adaptive_cont, -self.adaptive_switch)
            self.b -= self.adaptive_b
        elif self.optimization == 'adam':
            self.adam_v_dW = self.adam_beta1*self.adam_v_dW+(1-self.adam_beta1)*self.dW
            vdw=self.adam_v_dW/(1-self.adam_beta1**t)
            self.adam_v_db = self.adam_beta1*self.adam_v_db+(1-self.adam_beta1)*self.db
            vdb=self.adam_v_db/(1-self.adam_beta1**t)
            self.adam_s_dW = self.adam_beta2*self.adam_s_dW+(1-self.adam_beta2)*(self.dW)**2
            sdw=self.adam_s_dW/(1-self.adam_beta2**t)
            self.adam_s_db = self.adam_beta2*self.adam_s_db+(1-self.adam_beta2)*(self.db)**2
            sdb=self.adam_s_db/(1-self.adam_beta2**t)
            self.W-=(self.alpha*vdw)/np.sqrt(sdw+self.adam_epsilon)
            self.b-=(self.alpha*vdb)/np.sqrt(sdb+self.adam_epsilon)
        else:
            self.W -= self.alpha * self.dW
            self.b -= self.alpha * self.db



    @staticmethod
    def normal_initialization(shape,factor=0.01):
        W = np.random.randn(*shape)*factor
        return W

    def save_parameters(self, file_path):
        with h5py.File(f"{file_path}\\{self.name}.h5",'w') as hf:
            hf.create_dataset("W", data=self.W)
            hf.create_dataset("b", data=np.array(self.b))
    def restore_parameters(self, file_path):
        with h5py.File(f"{file_path}\\{self.name}.h5",'r') as hf:
            self.W=hf["W"][:]
            self.b=hf["b"][:]
    def parms_to_vec(self):
        return np.concatenate((np.reshape(self.W,(-1,)), np.reshape(self.b, (-1,))), axis=0)
    def vec_to_parms(self, vec):
        self.W = vec[0:self.W.size].reshape(self.W.shape)
        self.b = vec[self.W.size:].reshape(self.b.shape)
    def gradients_to_vec(self):
        return np.concatenate((np.reshape(self.dW,(-1,)), np.reshape(self.db, (-1,))), axis=0)

    def regularization_str(self):
        if self.regularization=="L2":
            s=f"regularization: {self.regularization}\n"
            s+="\tL2 parameters:\n"
            s+=f"\t\tlambda:{self.L2_lambda}\n"
        elif self.regularization=="dropout":
            s=f"regularization: {self.regularization}\n"
            s+="\tdropout parameters:\n"
            s+=f"\t\tkeep prob:{self.dropout_keep_prob}\n"
        else:
            return "\n"
        return s
    def forward_dropout(self,prev_A):
        copy_prev_a=np.copy(prev_A)
        if self.is_train==True and self.regularization=="dropout":
            self._D= np.random.rand(*prev_A.shape)
            self._D=np.where(self._D <= self.dropout_keep_prob, 1, 0)
            copy_prev_a*=self._D
            copy_prev_a/=self.dropout_keep_prob
        return copy_prev_a
    def backward_dropout(self, dA_prev):
        if self.regularization=="dropout":
            dA_prev*=self._D 
            dA_prev/=self.dropout_keep_prob
    def regularization_cost(self,m):
        if self.regularization=="L2":
            return self.L2_lambda*np.sum(np.square(self.W))/(2*m)
        else:
            return 0





class DLModel:
    def __init__(self,name,iTrainable, loss):
        self.inject_str_func = None
        self.loss=loss
        self.name = name
        self.inject_string_func = None
        self.iTrainable = iTrainable
        if loss=="square_dist":
            self.loss_forward=self.square_dist
            self.loss_backward=self.dSquare_dist
        elif loss=="cross_entropy":
            self.loss_forward=self.cross_entropy
            self.loss_backward=self.dCross_entropy
        elif loss=="categorical_cross_entropy":
            self.loss_forward=self.categorical_cross_entropy
            self.loss_backward=self.dCategorical_cross_entropy
        else:
            raise NotImplementedError("Error")


    def __str__(self):
        s = self.name + "\n"
        s += "\tLoss function: " + self.loss + "\n"
        s += "\t"+str(self.iTrainable) + "\n"
        return s


    def square_dist(self, Y_hat, Y):
        errors = (Y_hat - Y)**2
        return errors

    def dSquare_dist(self, Y_hat, Y):
        m = Y.shape[1]
        dY_hat = 2*(Y_hat - Y)/m
        return dY_hat

    def compute_cost(self, Y_hat, Y):
        m = Y.shape[1]
        errors = self.loss_forward(Y_hat, Y)
        J = np.sum(errors)/m+self.iTrainable.regularization_cost(m)
        return J


    def train(self, X, Y, num_epocs, mini_batch_size=64):
        self.iTrainable.set_train(True)
        print_ind = max(num_epocs//100, 1)
        costs = []
        t = 0
        if mini_batch_size <= 0:
            mini_batch_size = Y.shape[-1]
        for i in range(num_epocs):
            minibatch_iterator = DLModel.iterate_minibatches(X, Y, mini_batch_size)
            for minibatch in minibatch_iterator:
                Y_hat = self.forward_propagation(minibatch[0])
                self.backward_propagation(Y_hat, minibatch[1])
                t += 1
                self.update_parameters(t)  
            #record progress
            if i > 0 and i % print_ind == 0:
                J = self.compute_cost(Y_hat, minibatch[1])
                costs.append(J)
                #user defined info
                inject_string = ""
                if self.inject_str_func != None:
                    inject_string = self.inject_str_func(self, X, Y, Y_hat)
                print(f"cost after {i} full updates {100*i/num_epocs}%:{J}" + inject_string)
        costs.append(self.compute_cost(Y_hat, minibatch[1]))
        self.iTrainable.set_train(False)
        return costs



    def forward_propagation(self, X):
        return self.iTrainable.forward_propagation(X)

    def backward_propagation(self, Y_hat,Y):
        dY_hat = self.loss_backward(Y_hat, Y)
        self.iTrainable.backward_propagation(dY_hat)

    def update_parameters(self, t = 1):
        self.iTrainable.update_parameters()

    def cross_entropy(self, Y_hat, Y):
        eps = 1e-10
        Y_hat = np.where(Y_hat==0,eps,Y_hat)
        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)
        return(-(Y*np.log(Y_hat)+(1-Y)*np.log(1-Y_hat)))
    def dCross_entropy(self, Y_hat, Y):
        eps = 1e-10
        Y_hat = np.where(Y_hat==0,eps,Y_hat)
        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)
        m = Y.shape[1]
        return(-(Y/Y_hat) + (1-Y)/(1-Y_hat))/m
    def categorical_cross_entropy(self, Y_hat, Y): 
        eps = 1e-10
        Y_hat = np.where(Y_hat==0,eps,Y_hat)
        Y_hat = np.where(Y_hat == 1, 1-eps,Y_hat)
        Y_hat_np = np.log(Y_hat)
        mul= Y_hat_np*Y
        mul=-mul
        sum_mul= np.sum(mul,keepdims=True,axis=0)
        return sum_mul
    def dCategorical_cross_entropy(self, Y_hat, Y):
        m=Y.shape[1]
        dZ = (Y_hat- Y)/m
        return dZ
    @staticmethod
    def to_one_hot(num_categories, Y):
        m = Y.shape[0]
        Y = Y.reshape(1, m)
        Y_new = np.eye(num_categories)[Y.astype('int32')]
        Y_new = Y_new.T.reshape(num_categories, m)
        return Y_new
    def confusion_matrix(self, X, Y):
        prediction = self.forward_propagation(X)
        prediction_index = np.argmax(prediction, axis=0)
        Y_index = np.argmax(Y, axis=0)
        right = np.sum(prediction_index == Y_index)
        print("accuracy: ",str(right/len(Y[0])))
        print(confusion_matrix(prediction_index, Y_index))

    
    @staticmethod
    def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
        m = Y.shape[1]
        mini_batches = []
        np.random.seed(seed)
        permutation = list(np.random.permutation(m))
        shuffled_X = X[:, permutation]
        shuffled_Y = Y[:, permutation].reshape((-1,m))
        num_complete_minibatches = math.floor(m/mini_batch_size)
        for k in range(num_complete_minibatches):
            mini_batch_X = shuffled_X[:, mini_batch_size*k : (k+1) * mini_batch_size]
            mini_batch_Y = shuffled_Y[:, mini_batch_size*k : (k+1) * mini_batch_size]
            mini_batch = (mini_batch_X, mini_batch_Y)
            mini_batches.append(mini_batch)
        mini_size_n = m%mini_batch_size
        if mini_size_n != 0:
            mini_batch_X = shuffled_X[:, mini_size_n*(k+1) : (k+2) * mini_size_n]
            mini_batch_Y = shuffled_Y[:, mini_size_n*(k+1) : (k+2) * mini_size_n]
            mini_batch = (mini_batch_X, mini_batch_Y)
            mini_batches.append(mini_batch)
        return mini_batches


    @staticmethod
    def iterate_minibatches(inputs, targets, batchsize=64, shuffle=True):
        assert inputs.shape[-1] == targets.shape[-1]
        if shuffle:
            indices = np.arange(inputs.shape[-1])
            np.random.shuffle(indices)
        for start_idx in range(0, inputs.shape[-1], batchsize):
            if start_idx + batchsize >= inputs.shape[-1]:
                batchsize = inputs.shape[-1] - start_idx
            if shuffle:
                excerpt = indices[start_idx:start_idx + batchsize]
            else:
                excerpt = slice(start_idx, start_idx + batchsize)
            yield inputs.T[excerpt].T, targets.T[excerpt].T



class DLNeuronsLayer(DLNetwork):
    def __init__(self,name,num_units,input_size, activation, alpha,optimization=None, regularization = None):
        DLNetwork.__init__(self,name)
        self.linear = DLLinearLayer("linear",num_units,input_size, alpha,optimization, regularization)
        self.activation = DLActivation(activation)
        self.add(self.linear)
        self.add(self.activation)
        

